{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üé¨ IMDB Sentiment Analysis using Bidirectional LSTM\n",
        "\n",
        "[![Kaggle](https://img.shields.io/badge/Kaggle-Dataset-blue)](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)\n",
        "[![Python](https://img.shields.io/badge/Python-3.8+-green)](https://www.python.org/)\n",
        "[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-orange)](https://www.tensorflow.org/)\n",
        "\n",
        "## Project Overview\n",
        "\n",
        "This project implements a **Bidirectional LSTM (BiLSTM)** deep learning model for sentiment analysis on the IMDB movie reviews dataset. The model classifies reviews as either positive or negative, achieving **88% accuracy** on the test set.\n",
        "\n",
        "### Key Features\n",
        "- **Advanced NLP preprocessing** with negation word preservation\n",
        "- **Bidirectional LSTM architecture** for contextual understanding\n",
        "- **Early stopping** to prevent overfitting\n",
        "- **Comprehensive evaluation** with classification reports and confusion matrix\n",
        "\n",
        "### Dataset\n",
        "- **Source**: IMDB Dataset of 50K Movie Reviews\n",
        "- **Size**: 50,000 reviews (25,000 positive, 25,000 negative)\n",
        "- **Split**: 70% training, 15% validation, 15% testing\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ 1. Import Libraries and Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies if needed\n",
        "# !pip install kagglehub[pandas-datasets]\n",
        "\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the IMDB dataset from Kaggle\n",
        "print(\"Loading IMDB dataset from Kaggle...\")\n",
        "df = kagglehub.load_dataset(\n",
        "    KaggleDatasetAdapter.PANDAS,\n",
        "    \"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\",\n",
        "    \"\"\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Dataset loaded successfully!\")\n",
        "print(f\"\\nDataset shape: {df.shape}\")\n",
        "print(\"\\nFirst 5 records:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nDataset info:\")\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üßπ 2. Data Preprocessing & Tokenization\n",
        "\n",
        "### Preprocessing Pipeline:\n",
        "1. **Text Cleaning**: Lowercase, remove HTML tags, punctuation, extra spaces\n",
        "2. **Word Tokenization**: Split text into individual words\n",
        "3. **Stopword Removal**: Remove common words while **preserving negation words** (not, never, no, etc.)\n",
        "4. **Numerical Tokenization**: Convert text to numerical sequences\n",
        "5. **Padding**: Ensure uniform sequence length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# Step 1: Define Stopwords (excluding negation)\n",
        "# ==========================================\n",
        "\n",
        "# Load English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Negation words to preserve (critical for sentiment analysis)\n",
        "negation_words = {\n",
        "    \"no\", \"not\", \"nor\", \"never\", \"none\",\n",
        "    \"nobody\", \"nothing\", \"neither\", \"nowhere\",\n",
        "    \"cannot\", \"cant\"\n",
        "}\n",
        "\n",
        "# Remove negation words from stopwords\n",
        "stop_words = stop_words - negation_words\n",
        "\n",
        "print(f\"‚úÖ Stopwords loaded: {len(stop_words)} words\")\n",
        "print(f\"‚úÖ Negation words preserved: {negation_words}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# Step 2: Text Cleaning Function\n",
        "# ==========================================\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Clean and preprocess text data.\n",
        "    \n",
        "    Args:\n",
        "        text (str): Raw text input\n",
        "    \n",
        "    Returns:\n",
        "        str: Cleaned text\n",
        "    \"\"\"\n",
        "    text = text.lower()                      # Convert to lowercase\n",
        "    text = re.sub(r\"<.*?>\", \"\", text)        # Remove HTML tags\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # Remove punctuation and numbers\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip() # Remove extra whitespace\n",
        "    return text\n",
        "\n",
        "\n",
        "# Apply text cleaning\n",
        "print(\"Cleaning text data...\")\n",
        "df['clean_review'] = df['review'].apply(clean_text)\n",
        "print(\"‚úÖ Text cleaning completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# Step 3: Word Tokenization\n",
        "# ==========================================\n",
        "\n",
        "print(\"Tokenizing words...\")\n",
        "df['tokens'] = df['clean_review'].apply(word_tokenize)\n",
        "print(\"‚úÖ Word tokenization completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# Step 4: Remove Stopwords (preserve negation)\n",
        "# ==========================================\n",
        "\n",
        "print(\"Removing stopwords (preserving negation)...\")\n",
        "df['tokens'] = df['tokens'].apply(\n",
        "    lambda tokens: [word for word in tokens if word not in stop_words]\n",
        ")\n",
        "print(\"‚úÖ Stopword removal completed!\")\n",
        "\n",
        "# Display sample\n",
        "print(\"\\nSample of preprocessed reviews:\")\n",
        "print(df[['review', 'tokens']].head(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# Step 5: Numerical Tokenization & Padding\n",
        "# ==========================================\n",
        "\n",
        "# Join tokens back into strings\n",
        "df['final_review'] = df['tokens'].apply(lambda x: \" \".join(x))\n",
        "\n",
        "# Configuration\n",
        "VOCAB_SIZE = 10000    # Maximum vocabulary size\n",
        "MAX_LENGTH = 200      # Maximum sequence length\n",
        "\n",
        "# Initialize and fit tokenizer\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(df['final_review'])\n",
        "\n",
        "# Convert texts to sequences\n",
        "sequences = tokenizer.texts_to_sequences(df['final_review'])\n",
        "\n",
        "# Pad sequences to uniform length\n",
        "X = pad_sequences(\n",
        "    sequences,\n",
        "    maxlen=MAX_LENGTH,\n",
        "    padding='post',\n",
        "    truncating='post'\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Numerical tokenization completed!\")\n",
        "print(f\"Vocabulary size: {VOCAB_SIZE}\")\n",
        "print(f\"Maximum sequence length: {MAX_LENGTH}\")\n",
        "print(f\"\\nData shape after preprocessing: {X.shape}\")\n",
        "print(f\"Sample sequence: {X[0][:20]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä 3. Label Encoding & Train-Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Encode sentiment labels (positive=1, negative=0)\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(df['sentiment'])\n",
        "\n",
        "print(f\"Label encoding: {dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))}\")\n",
        "print(f\"\\nClass distribution:\")\n",
        "print(pd.Series(y).value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data: 70% train, 15% validation, 15% test\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Data split completed!\")\n",
        "print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† 4. Build Bidirectional LSTM Model\n",
        "\n",
        "### Model Architecture:\n",
        "1. **Embedding Layer**: Converts word indices to dense vectors (128 dimensions)\n",
        "2. **Bidirectional LSTM**: Processes sequences in both forward and backward directions (64 units)\n",
        "3. **Dropout Layer**: Prevents overfitting (50% dropout rate)\n",
        "4. **Dense Output Layer**: Sigmoid activation for binary classification\n",
        "\n",
        "### Why Bidirectional LSTM?\n",
        "- Captures **context from both directions** in the sequence\n",
        "- Better understanding of sentiment nuances\n",
        "- Improved performance on long-term dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Model configuration\n",
        "EMBEDDING_DIM = 128\n",
        "LSTM_UNITS = 64\n",
        "DROPOUT_RATE = 0.5\n",
        "\n",
        "# Build the model\n",
        "model = Sequential([\n",
        "    # Embedding layer: converts word indices to dense vectors\n",
        "    Embedding(\n",
        "        input_dim=VOCAB_SIZE,\n",
        "        output_dim=EMBEDDING_DIM,\n",
        "        input_length=MAX_LENGTH\n",
        "    ),\n",
        "    \n",
        "    # Bidirectional LSTM: processes sequences in both directions\n",
        "    Bidirectional(LSTM(LSTM_UNITS)),\n",
        "    \n",
        "    # Dropout: prevents overfitting\n",
        "    Dropout(DROPOUT_RATE),\n",
        "    \n",
        "    # Output layer: binary classification\n",
        "    Dense(1, activation='sigmoid')\n",
        "], name='BiLSTM_Sentiment_Classifier')\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Display model architecture\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL ARCHITECTURE\")\n",
        "print(\"=\"*60)\n",
        "model.summary()\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ 5. Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Early stopping callback\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=2,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Training configuration\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING MODEL\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Epochs: {EPOCHS}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Early stopping: Enabled (patience=2)\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Training completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà 6. Model Evaluation\n",
        "\n",
        "Evaluate the model performance on the test set using:\n",
        "- **Classification Report**: Precision, Recall, F1-Score\n",
        "- **Confusion Matrix**: True Positives, False Positives, True Negatives, False Negatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Make predictions\n",
        "print(\"Making predictions on test set...\")\n",
        "y_pred_prob = model.predict(X_test, verbose=0)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CLASSIFICATION REPORT\")\n",
        "print(\"=\"*60)\n",
        "print(classification_report(\n",
        "    y_test, y_pred,\n",
        "    target_names=['Negative', 'Positive'],\n",
        "    digits=4\n",
        "))\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CONFUSION MATRIX\")\n",
        "print(\"=\"*60)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "print(\"\\n[Row = Actual, Column = Predicted]\")\n",
        "print(f\"True Negatives: {cm[0][0]}\")\n",
        "print(f\"False Positives: {cm[0][1]}\")\n",
        "print(f\"False Negatives: {cm[1][0]}\")\n",
        "print(f\"True Positives: {cm[1][1]}\")\n",
        "\n",
        "# Calculate test accuracy\n",
        "test_accuracy = (cm[0][0] + cm[1][1]) / cm.sum()\n",
        "print(f\"\\nüéØ Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üé® 7. Visualize Results (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Accuracy plot\n",
        "axes[0].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
        "axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
        "axes[0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Loss plot\n",
        "axes[1].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
        "axes[1].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "axes[1].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(\n",
        "    cm, annot=True, fmt='d', cmap='Blues',\n",
        "    xticklabels=['Negative', 'Positive'],\n",
        "    yticklabels=['Negative', 'Positive'],\n",
        "    cbar_kws={'label': 'Count'}\n",
        ")\n",
        "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÆ 8. Test with Custom Reviews (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_sentiment(review_text):\n",
        "    \"\"\"\n",
        "    Predict sentiment of a custom review.\n",
        "    \n",
        "    Args:\n",
        "        review_text (str): Movie review text\n",
        "    \n",
        "    Returns:\n",
        "        str: Predicted sentiment (Positive/Negative) with confidence\n",
        "    \"\"\"\n",
        "    # Preprocess\n",
        "    cleaned = clean_text(review_text)\n",
        "    tokens = word_tokenize(cleaned)\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    final = \" \".join(tokens)\n",
        "    \n",
        "    # Tokenize and pad\n",
        "    seq = tokenizer.texts_to_sequences([final])\n",
        "    padded = pad_sequences(seq, maxlen=MAX_LENGTH, padding='post')\n",
        "    \n",
        "    # Predict\n",
        "    prob = model.predict(padded, verbose=0)[0][0]\n",
        "    sentiment = \"Positive\" if prob > 0.5 else \"Negative\"\n",
        "    confidence = prob if prob > 0.5 else 1 - prob\n",
        "    \n",
        "    return f\"{sentiment} ({confidence*100:.2f}% confidence)\"\n",
        "\n",
        "\n",
        "# Test with sample reviews\n",
        "test_reviews = [\n",
        "    \"This movie was absolutely amazing! I loved every minute of it.\",\n",
        "    \"Terrible film. Waste of time and money. Do not watch.\",\n",
        "    \"Not bad, but could have been better. Average at best.\"\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CUSTOM REVIEW PREDICTIONS\")\n",
        "print(\"=\"*60)\n",
        "for i, review in enumerate(test_reviews, 1):\n",
        "    prediction = predict_sentiment(review)\n",
        "    print(f\"\\nReview {i}: \\\"{review}\\\"\")\n",
        "    print(f\"Prediction: {prediction}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Results Summary\n",
        "\n",
        "### Model Performance\n",
        "- **Test Accuracy**: ~88%\n",
        "- **Precision**: 0.87-0.89 for both classes\n",
        "- **Recall**: 0.87-0.89 for both classes\n",
        "- **F1-Score**: 0.88 (balanced performance)\n",
        "\n",
        "### Key Achievements\n",
        "‚úÖ Successfully implemented BiLSTM architecture for sentiment analysis  \n",
        "‚úÖ Preserved negation words to maintain sentiment context  \n",
        "‚úÖ Applied early stopping to prevent overfitting  \n",
        "‚úÖ Achieved balanced performance across both classes  \n",
        "\n",
        "### Potential Improvements\n",
        "- Experiment with GRU layers as an alternative to LSTM\n",
        "- Try pre-trained word embeddings (GloVe, Word2Vec)\n",
        "- Implement attention mechanisms\n",
        "- Increase model depth (add more LSTM layers)\n",
        "- Apply data augmentation techniques\n",
        "\n",
        "---\n",
        "\n",
        "## üìù Citation\n",
        "\n",
        "**Dataset**: Maas et al. (2011) - IMDB Movie Reviews Dataset  \n",
        "**Kaggle**: [lakshmi25npathi/imdb-dataset-of-50k-movie-reviews](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)\n",
        "\n",
        "---\n",
        "\n",
        "**Author**: Mohmad Taha Jasem Alhmad  \n",
        "**GitHub**: [Your GitHub Profile]  \n",
        "**Date**: February 2026  "
      ]
    }
  ]
}
